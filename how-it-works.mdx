---
title: "How It Works"
description: "The architecture behind AgentReady: what happens when an AI agent visits your site vs. a browser."
---

## Two Types of Visitors

Your site receives two fundamentally different types of traffic:

| | Human (Browser) | Agent (AI Crawler) |
|---|---|---|
| **Sends** | `Accept: text/html` | `Accept: text/markdown` |
| **Needs** | Layout, styling, interactivity | Semantic content only |
| **Token cost** | N/A | ~50,000 tokens per page |
| **With AgentReady** | Unchanged | ~200 tokens per page |

AgentReady detects which type of visitor is making the request and serves the appropriate format. Human visitors are never affected.

## The Request Flow

### Human Visit (Unchanged)

```
Browser → GET /pricing
         Accept: text/html
         User-Agent: Mozilla/5.0...

  1. proxy.ts runs
  2. withAgentServing(request) → checks Accept header → text/html → returns null
  3. Falls through to your normal Clerk → i18n → Next.js rendering
  4. Browser gets the full HTML page

Result: Zero impact. Same page as before.
```

### Agent Visit (New Behavior)

```
Agent → GET /pricing
        Accept: text/markdown, text/plain, text/html
        User-Agent: ClaudeBot/1.0

  1. proxy.ts runs
  2. withAgentServing(request) → detects text/markdown before text/html
  3. Internally fetches GET /pricing as HTML from your own server
  4. Parses HTML with linkedom (no browser needed)
  5. Strips: nav, footer, script, style, cookie banners
  6. Converts remaining DOM to markdown via turndown
  7. Prepends llms.txt discovery block
  8. Logs request asynchronously (non-blocking)
  9. Returns text/markdown response

Result: 500KB HTML → 2KB markdown. 99.6% reduction.
```

The key insight is that AgentReady doesn't need a separate content source. It fetches your own rendered HTML and converts it on the fly. Your existing Next.js caching, ISR, and SSG all work exactly as before — AgentReady just transforms the output for a different audience.

## The Three Layers

AgentReady operates across three layers, each handling a different concern.

### Layer 1: Config Wrapper (Build Time)

`withAgentReady(nextConfig)` runs once when your Next.js config is loaded. It makes three non-destructive modifications:

- **Response headers** — Adds `Link: </llms.txt>; rel="llms-txt"` to every page, so any agent visiting any page can discover your llms.txt from the headers alone
- **Rewrites** — Maps `/llms.txt` and `/llms-full.txt` to internal API routes, prepended before your existing rewrites so they don't conflict with API proxies
- **Environment variables** — Serializes config so the proxy helper has access at runtime

This layer follows the same `(NextConfig) => NextConfig` pattern as `withNextIntl` and `withSentryConfig`, which is why it composes cleanly with any number of config wrappers.

### Layer 2: Proxy Helper (Request Time)

`withAgentServing(request)` runs on every request and does one of two things:

- **Returns `null`** if the visitor is human — your middleware continues normally
- **Returns a `Response`** if the visitor is an agent — the response is markdown

The detection logic is two-layered: first it checks the `Accept` header (does `text/markdown` appear before `text/html`?), then falls back to User-Agent matching against known AI crawlers. Accept header detection is preferred because it's standards-based and explicit — see [Content Negotiation](/concepts/content-negotiation) for the reasoning behind this design.

### Layer 3: Conversion Pipeline (On Demand)

When an agent request is detected, the conversion pipeline runs:

```
Your HTML page
    │
    ▼
linkedom (parse HTML into DOM without a browser)
    │
    ▼
Strip selectors (nav, footer, script, style, [data-clerk], .cookie-banner)
    │
    ▼
turndown (convert DOM to markdown)
    │
    ▼
Prepend llms.txt discovery block
    │
    ▼
Return as text/markdown with proper headers
```

The pipeline uses [linkedom](https://github.com/WebReflection/linkedom) instead of a headless browser — it's a lightweight DOM implementation that parses HTML in ~10ms. Combined with turndown for HTML-to-markdown conversion, the entire pipeline typically completes in under 80ms on the first request and is cached after that.

## Why This Architecture?

A few design decisions worth understanding:

**Why internal fetch instead of a separate content source?** Because your pages already contain the right content. Re-creating a parallel content pipeline would be fragile and expensive. By fetching your own rendered HTML, AgentReady gets the benefit of your existing CMS, database queries, ISR, and caching — all for free.

**Why function-based instead of creating middleware?** Next.js only allows one middleware file. A library that creates `middleware.ts` (like [accept-md](https://github.com/nicholasgriffintn/accept-md)) immediately conflicts with Clerk, next-intl, or any other middleware. AgentReady provides a function you call inside your existing handler, which is why it composes with any middleware stack. See [Middleware Composition](/guides/middleware-composition) for patterns.

**Why `waitUntil()` for logging?** The agent gets their markdown immediately. Logging happens in the background after the response is sent, so there's zero latency impact. See [Observability](/concepts/observability) for what gets logged and why.

## The llms.txt Standard

AgentReady automatically generates `/llms.txt` from your pages and sitemap. This follows the [llms.txt specification](https://llmstxt.org/) — a proposed standard for helping AI agents discover and navigate website content. Think of it as `robots.txt` for AI: instead of telling crawlers what *not* to access, it tells them what's *available* and how to find it.

The file is auto-generated and updates when your pages change. See [llms.txt](/concepts/llms-txt) for the full spec, customization options, and adoption data.
